# Placeholder for runtime configuration (a100) 

# @package _group_

# Runtime and Trainer settings optimized for A100 (BF16)

# Directory for logs, checkpoints, etc. (relative to hydra run dir)
# Often overridden by SLURM scripts to use $TMPDIR
output_dir: "outputs/"

# --- PyTorch Lightning Trainer / HF Trainer Arguments ---
training:
  # Batching & Accumulation
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16 # Can often be larger for eval
  gradient_accumulation_steps: 4  # Effective batch size = 8 * 4 * num_gpus

  # Training Duration & Scheduling
  num_train_epochs: 3
  learning_rate: 2.0e-4 # 0.0002
  lr_scheduler_type: "linear"
  warmup_ratio: 0.05

  # Precision (must match model config)
  fp16: false
  bf16: true

  # Checkpointing & Evaluation Strategy
  load_best_model_at_end: true
  metric_for_best_model: "eval_ner_f1" # Primary metric (computed by callback)
  greater_is_better: true
  evaluation_strategy: "steps"
  eval_steps: 500           # Evaluate every N steps
  save_strategy: "steps"    # Corresponds to evaluation_strategy
  save_steps: ${.eval_steps} # Save checkpoint every time evaluation runs
  save_total_limit: 2       # Keep only the best and the last checkpoint

  # Logging
  logging_strategy: "steps"
  logging_steps: 50         # Log training loss frequently
  report_to: ["wandb"]     # Report metrics to W&B

  # Performance & Distributed Training
  gradient_checkpointing: ${model.gradient_checkpointing} # Link to model config
  ddp_find_unused_parameters: false # Often needed with grad checkpointing
  # torch_compile: false # Optional: Experiment if beneficial

  # Generation Settings during Evaluation (controlled by callback/main script)
  # predict_with_generate: false # Default set to false, enabled selectively
  # generation_max_new_tokens: ${model.generation_config.max_new_tokens}
  # generation_num_beams: 1 # Default greedy

  # --- Optional DeepSpeed --- (disabled by default)
  # deepspeed: null # Path to deepspeed config, e.g., conf/deepspeed/zero2_offload.json

# W&B Configuration
wandb:
  project: "TaskMerge" # Default project name (can be overridden)
  # name: null # Run name (defaults to hydra job name)
  # entity: null # W&B entity (optional)
  log_model: "checkpoint" # Log model checkpoints as W&B artifacts 