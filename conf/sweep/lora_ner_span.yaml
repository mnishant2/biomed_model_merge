# W&B Sweep configuration for NER Span LoRA tuning

method: grid # Can change to 'bayes' or 'random' for more advanced search

metric:
  name: eval_final_ner_f1 # Target metric logged by main script
  goal: maximize

parameters:
  # Learning rate for AdamW optimizer
  runtime.training.learning_rate:
    values: [1.0e-4, 2.0e-4, 3.0e-4] # Values from spec

  # LoRA rank (r)
  model.lora_r:
    values: [4, 8, 16] # Values from spec

  # LoRA dropout rate
  model.lora_dropout:
    values: [0.05, 0.1] # Values from spec

  # Subsample fraction (fixed for this sweep example)
  task.datamodule.train_fraction:
    values: [0.2] # Fixed value from spec (can add more)

# --- Command template for W&B agent ---
# This command will be executed by each sweep agent.
# It launches the main training script with parameters injected by W&B.

# Note: W&B injects parameters like ${parameter_name}
# Hydra overrides use syntax like parameter.name=value or ++parameter.name=value

command:
  - ${env}      # Environment variables (like CUDA_VISIBLE_DEVICES)
  - python      # Or torchrun if using multi-gpu per agent
  - -m
  - taskmerge.src.main # Path to your main training script module
  - task=ner_span
  - model=unsloth_lora_bf16
  - runtime=a100        # Base runtime, specific params overridden below
  # --- W&B Injected Parameters ---
  # Use Hydra's override syntax (++ ensures it takes precedence)
  - ++runtime.training.learning_rate=${runtime.training.learning_rate}
  - ++model.lora_r=${model.lora_r}
  - ++model.lora_dropout=${model.lora_dropout}
  - ++task.datamodule.train_fraction=${task.datamodule.train_fraction}
  # --- Fixed paths for sweep runs (using WORKROOT, adjust if needed) ---
  # Assuming agent runs from WORKROOT. Point data/output to persistent/shared locations
  # or manage via the agent execution environment (e.g., SLURM script copies data).
  # Using WORKROOT paths directly here assumes shared filesystem access.
  - task.datamodule.data_dir=taskmerge/data/union_span/ner # Use persistent data for sweep
  - model.tokenizer_path=taskmerge/tokenizers/llama3_biomerge_tok # Use persistent tokenizer
  # Let Hydra manage output dirs based on overrides for each run
  # - runtime.training.output_dir=... # Default hydra output path is usually fine
  - ${args_no_hyphens} # Pass other W&B agent args 