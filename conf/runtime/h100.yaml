# Placeholder for runtime configuration (h100) 

# @package _group_

# Runtime and Trainer settings potentially optimized for H100 (BF16)
# Start by copying A100 settings and adjust if needed (e.g., larger batch size)

_target_: taskmerge.conf.runtime.a100.Config # Inherit from A100 config

# Directory for logs, checkpoints, etc. (relative to hydra run dir)
# Often overridden by SLURM scripts to use $TMPDIR
output_dir: "outputs/"

# --- PyTorch Lightning Trainer / HF Trainer Arguments ---
training:
  # Batching & Accumulation (Potentially increase batch size for H100)
  per_device_train_batch_size: 16 # Example: Double batch size for H100
  per_device_eval_batch_size: 32 # Example: Double eval batch size
  gradient_accumulation_steps: 2  # Adjust accumulation to maintain similar effective batch size (16 * 2 * num_gpus)

  # Training Duration & Scheduling (Keep same as A100 for now)
  num_train_epochs: 3
  learning_rate: 2.0e-4
  lr_scheduler_type: "linear"
  warmup_ratio: 0.05

  # Precision (H100 excellent with BF16)
  fp16: false
  bf16: true

  # Checkpointing & Evaluation Strategy (Keep same as A100)
  load_best_model_at_end: true
  metric_for_best_model: "eval_ner_f1"
  greater_is_better: true
  evaluation_strategy: "steps"
  eval_steps: 500
  save_strategy: "steps"
  save_steps: ${.eval_steps}
  save_total_limit: 2

  # Logging (Keep same as A100)
  logging_strategy: "steps"
  logging_steps: 50
  report_to: ["wandb"]

  # Performance & Distributed Training (Keep same as A100)
  gradient_checkpointing: ${model.gradient_checkpointing}
  ddp_find_unused_parameters: false
  # Consider enabling torch_compile: true for potential H100 speedups
  # torch_compile: true

  # --- Optional DeepSpeed --- (disabled by default)
  # deepspeed: null

# W&B Configuration (Inherited from A100)
wandb:
  project: "TaskMerge"
  log_model: "checkpoint" 